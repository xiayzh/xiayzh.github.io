<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <title>RGBD Segmentation</title>

  <meta name="viewport" content="width=device-width">
</head>

<body>
  <div class="wrapper" style="font-family: Helvetica, Tahoma, Arial; width:960px; margin: 0 auto; line-height:150%">

    <section>

      <h3>
        <a id="Back-pages" class="anchor" href="#Back-pages" aria-hidden="true"><span
            class="octicon octicon-link"></span></a><a href="index.html#Project-page">
          <ud>Back to Homepage</ud>
        </a> </h3>

      <h2>
        <a id="project_title" class="anchor" href="#project_title" aria-hidden="true"><span
            class="octicon octicon-link"></span></a>Object-based RGBD foreground segmentation</h2>




      <h4>
        <a id="Introduction-page" class="anchor" href="#Introduction-pages" aria-hidden="true"><span
            class="octicon octicon-link"></span></a>Introduction:</h4>

      <p>We present an object-based co-segmentation method that takes advantage of depth data and is able to correctly
        handle noisy images in which the common foreground object is missing. With RGBD images, our method utilizes the
        depth channel to enhance identification of similar foreground objects via a proposed RGBD co-saliency map, as
        well as to improve detection of object-like regions and provide depth- based local features for region
        comparison. To accurately deal with noisy images where the common object appears more than or less than once, we
        formulate co-segmentation in a fully-connected graph structure together with mutual exclusion (mutex)
        constraints that prevent improper solutions. Experiments show that this object-based RGBD co-segmentation with
        mutex constraints outperforms related techniques on an RGBD co-segmentation dataset, while effectively
        processing noisy images. Moreover, we show that this method also provides performance comparable to state-
        of-the-art RGB co-segmentation techniques on regular RGB images with depth maps estimated from them.</p>

      <div style="text-align: center; display: block; margin-right: auto;">
        <img src="sub_img/rgbd_seg_framework.jpg" border="0" width="600"><br></div><br>


      <hr />
      <h4>Paper:</h4>

      <p>[1] Huazhu Fu, Dong Xu, Stephen Lin, Jiang Liu, <strong>"Object-based RGBD Image Co-segmentation with Mutex
          Constraint"</strong>, in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015, pp.
        4428-4436. <br>
        <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Fu_Object-Based_RGBD_Image_2015_CVPR_paper.html"
          target="_blank">[PDF]</a> <br>

        [2] Huazhu Fu, Dong Xu, Stephen Lin, <strong>"Object-based Multiple Foreground Segmentation in RGBD
          Video"</strong>, in <em>IEEE Transactions on Image Processing (TIP)</em>, vol. 26, no. 3, pp. 1418-1427, 2017.
        <br> <a href="http://dx.doi.org/10.1109/TIP.2017.2651369" target="_blank">[PDF]</a>
      </p>

      <hr />
      <h4>RGBD image co-segmentation dataset:</h4>
      <p> We build a RGBD image co-segmentation dataset, which contains 16 image sets, each of 6 to 17 images taken from
        indoor scenes with one common foreground object (193 images in total): <br>
        RGBD image co-segmentation dataset (~102MB), download: <a
          href="https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!269&authkey=!AHUBN0lk5kQLWzQ&ithint=file%2czip"
          target="_blank">
          <font color="#ff0000">[OneDrive]</font>
        </a> <a href="https://pan.baidu.com/s/1cAMin0" target="_blank">
          <font color="#ff0000">[BaiduYun]</font>
        </a>. <br>
        And our result (~20MB), download: <a
          href="https://onedrive.live.com/redir?resid=F3A8A31ABFAC51B0!255&authkey=!ANuwbADPp9Dsm4Q&ithint=file%2czip">
          <font color="#ff0000">[OneDrive]</font>
        </a> <a href="https://pan.baidu.com/s/1o7TKp8Q">
          <font color="#ff0000">[BaiduYun]</font>
        </a>.</p>

      <h4>RGBD video foreground segmentation dataset:</h4>
      <p> We also build a RGBD video foreground segmentation dataset for journal version, which contains 11 videos, each
        of 20 to 56 frames with one or two foregrounds: <br>
        RGBD video segmentation dataset and our result (~332MB), download: <a
          href="https://1drv.ms/u/s!ArBRrL8ao6jzhmX6SC75hVJppb3D" target="_blank">
          <font color="#ff0000">[OneDrive]</font>
        </a> <a href="https://pan.baidu.com/s/1eSFxIGa" target="_blank">
          <font color="#ff0000">[BaiduYun]</font>
        </a>.</p>

      <hr />
      <h4>Related Works:</h4>

      <p>[1] Huazhu Fu, Dong Xu, Bao Zhang, Stephen Lin, Rabab K. Ward, <a
          href="http://dx.doi.org/10.1109/TIP.2015.2442915" target="_blank"><strong>"Object-based Multiple Foreground
            Video Co-segmentation via Multi-state Selection Graph"</strong></a>, <em>IEEE Transactions on Image
          Processing (TIP)</em>, vol. 24, no. 11, pp. 3415-3424, 2015.
        [<a href="https://github.com/HzFu/VideoCoSeg_MSG" target="_blank">
          <font color="#ff0000">Code</font>
        </a>] [<a href="proj_video_coseg.html" target="_blank">
          <font color="#ff0000">Project</font>
        </a>] </p>

      <p>[2] Huazhu Fu, Xiaochun Cao, Zhuowen Tu, <a href="http://dx.doi.org/10.1109%2FTIP.2013.2260166"
          target="_blank"><strong>"Cluster-based Co-saliency Detection"</strong></a>, <em>IEEE Transactions on Image
          Processing (TIP)</em>, vol. 22, no. 10, pp. 3766-3778, 2013.
        [<a href="https://github.com/HzFu/Cosaliency_tip2013" target="_blank">
          <font color="#ff0000">Code</font>
        </a>] </p>

      <p>[3] Xiaochun Cao, Zhiqiang Tao, Bao Zhang, Huazhu Fu, Wei Feng, <a
          href="http://dx.doi.org/10.1109/TIP.2014.2332399" target="_blank"><strong>"Self-adaptively Weighted
            Co-saliency Detection via Rank Constraint"</strong></a>, <em>IEEE Transactions on Image Processing
          (TIP)</em>, vol. 23, no. 9, pp. 4175-4186, 2014.
        [<a href="https://github.com/HzFu/SACS_TIP2014" target="_blank">
          <font color="#ff0000">Code</font>
        </a>]</p>

    </section>

  </div>

</body>

</html>